{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New baseline for the bovine with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired from this guide :\n",
    "https://keras.io/examples/vision/video_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data, get_test_data, WeightedClassificationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#CPU\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  \n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import imageio\n",
    "import ipywidgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (30, 226, 226, 1)\n",
    "NUM_CLASSES = 8\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 160\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (9, 60, 60)\n",
    "STRIDES= tuple(list(map(lambda x:x//2, PATCH_SIZE)))\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 30, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STRIDES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_train, labels_train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_classifier= np.array(videos_train)\n",
    "y_for_classifier= labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_test, labels_test  = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_for_classifier = np.array(videos_test)\n",
    "ytest_for_classifier = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_frames(video):\n",
    "    res=[]\n",
    "    for frame in video:\n",
    "        resized_img=Image.fromarray(frame).resize(INPUT_SHAPE[1:3])\n",
    "        res.append(np.array(resized_img))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets all dataset\n",
    "# 30 frames per video for 177 video = 2.65 gb !if considering each frame of float64\n",
    "# as uint8 it takes 0.33 gb\n",
    "\n",
    "def gen_videos(videolist):\n",
    "    newvideos=[] # 177*30*250*250\n",
    "    for video in videolist:\n",
    "        reducedvideo= video.read_samples(video.frame_times[0:299:10])\n",
    "        reducedvideo= reducedvideo.astype('uint8')        \n",
    "        #CROP from 250 to 224\n",
    "        reducedvideo=resize_frames(reducedvideo)\n",
    "        #and add a batch dimension. dim= 1*30*250*250*3\n",
    "        #reducedvideo = reducedvideo[None, ...]\n",
    "\n",
    "        newvideos.append(reducedvideo)\n",
    "    return newvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 30, 226, 226)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_for_classifier= np.array(gen_videos(X_for_classifier))\n",
    "X_for_classifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30, 226, 226)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_for_classifier= np.array(gen_videos(Xtest_for_classifier))\n",
    "Xtest_for_classifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 30, 226, 226)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_videos=X_for_classifier\n",
    "test_videos=Xtest_for_classifier\n",
    "train_videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_int(argument):\n",
    "    switcher = {\n",
    "        'A':0,\n",
    "        'B':1,\n",
    "        'C':2,\n",
    "        'D':3,\n",
    "        'E':4,\n",
    "        'F':5,\n",
    "        'G':6,\n",
    "        'H':7,\n",
    "    }\n",
    " \n",
    "    # get() method of dictionary data type returns\n",
    "    # value of passed argument if it is present\n",
    "    # in dictionary otherwise second argument will\n",
    "    # be assigned as default value of passed argument\n",
    "    return switcher.get(argument, \"nothing\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "func=np.vectorize(class_to_int)\n",
    "#Train\n",
    "train_labels=func(y_for_classifier)\n",
    "#Test\n",
    "test_labels=func(ytest_for_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(trainloader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size,strides, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=strides,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 6s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape=INPUT_SHAPE)\n",
    "out = TubeletEmbedding(\n",
    "    embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE, strides=STRIDES\n",
    ")(inp)\n",
    "model = Model(inp, out)\n",
    "\n",
    "output = model.predict(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 216, 128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "6/6 [==============================] - 11s 171ms/step - loss: 2.2065 - accuracy: 0.2034 - top-5-accuracy: 0.7345\n",
      "Epoch 2/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 2.0570 - accuracy: 0.2486 - top-5-accuracy: 0.7458\n",
      "Epoch 3/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 2.0143 - accuracy: 0.2147 - top-5-accuracy: 0.7684\n",
      "Epoch 4/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.9961 - accuracy: 0.2260 - top-5-accuracy: 0.7853\n",
      "Epoch 5/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9972 - accuracy: 0.2203 - top-5-accuracy: 0.7740\n",
      "Epoch 6/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9865 - accuracy: 0.2147 - top-5-accuracy: 0.7571\n",
      "Epoch 7/160\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 1.9763 - accuracy: 0.2429 - top-5-accuracy: 0.8136\n",
      "Epoch 8/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.9710 - accuracy: 0.2486 - top-5-accuracy: 0.7966\n",
      "Epoch 9/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9644 - accuracy: 0.2486 - top-5-accuracy: 0.8136\n",
      "Epoch 10/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9531 - accuracy: 0.2486 - top-5-accuracy: 0.8192\n",
      "Epoch 11/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9547 - accuracy: 0.2486 - top-5-accuracy: 0.8136\n",
      "Epoch 12/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.9554 - accuracy: 0.2486 - top-5-accuracy: 0.8079\n",
      "Epoch 13/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9448 - accuracy: 0.2486 - top-5-accuracy: 0.8136\n",
      "Epoch 14/160\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 1.9469 - accuracy: 0.2486 - top-5-accuracy: 0.7966\n",
      "Epoch 15/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9441 - accuracy: 0.2486 - top-5-accuracy: 0.8136\n",
      "Epoch 16/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.9438 - accuracy: 0.2486 - top-5-accuracy: 0.8249\n",
      "Epoch 17/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.9371 - accuracy: 0.2486 - top-5-accuracy: 0.8192\n",
      "Epoch 18/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.9581 - accuracy: 0.1525 - top-5-accuracy: 0.8136\n",
      "Epoch 19/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.9356 - accuracy: 0.2486 - top-5-accuracy: 0.8305\n",
      "Epoch 20/160\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 1.9390 - accuracy: 0.2486 - top-5-accuracy: 0.8249\n",
      "Epoch 21/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.9377 - accuracy: 0.2712 - top-5-accuracy: 0.8249\n",
      "Epoch 22/160\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 1.9329 - accuracy: 0.2599 - top-5-accuracy: 0.8192\n",
      "Epoch 23/160\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 1.9288 - accuracy: 0.2486 - top-5-accuracy: 0.8362\n",
      "Epoch 24/160\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 1.9326 - accuracy: 0.2429 - top-5-accuracy: 0.8249\n",
      "Epoch 25/160\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 1.9275 - accuracy: 0.2486 - top-5-accuracy: 0.8023\n",
      "Epoch 26/160\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 1.9351 - accuracy: 0.2486 - top-5-accuracy: 0.8305\n",
      "Epoch 27/160\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 1.9438 - accuracy: 0.1921 - top-5-accuracy: 0.8192\n",
      "Epoch 28/160\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 1.9082 - accuracy: 0.2938 - top-5-accuracy: 0.8305\n",
      "Epoch 29/160\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 1.9408 - accuracy: 0.2486 - top-5-accuracy: 0.8192\n",
      "Epoch 30/160\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 1.9210 - accuracy: 0.2316 - top-5-accuracy: 0.8079\n",
      "Epoch 31/160\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 1.9193 - accuracy: 0.2599 - top-5-accuracy: 0.8305\n",
      "Epoch 32/160\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 1.9151 - accuracy: 0.2542 - top-5-accuracy: 0.8136\n",
      "Epoch 33/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.9270 - accuracy: 0.2429 - top-5-accuracy: 0.8475\n",
      "Epoch 34/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.9177 - accuracy: 0.2768 - top-5-accuracy: 0.8136\n",
      "Epoch 35/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.9091 - accuracy: 0.2429 - top-5-accuracy: 0.8305\n",
      "Epoch 36/160\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 1.9268 - accuracy: 0.2147 - top-5-accuracy: 0.8362\n",
      "Epoch 37/160\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 1.9153 - accuracy: 0.2655 - top-5-accuracy: 0.8362\n",
      "Epoch 38/160\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 1.9090 - accuracy: 0.2599 - top-5-accuracy: 0.8644\n",
      "Epoch 39/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.9254 - accuracy: 0.2542 - top-5-accuracy: 0.8531\n",
      "Epoch 40/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.9106 - accuracy: 0.2881 - top-5-accuracy: 0.8475\n",
      "Epoch 41/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.9267 - accuracy: 0.2373 - top-5-accuracy: 0.8249\n",
      "Epoch 42/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.9009 - accuracy: 0.2938 - top-5-accuracy: 0.7853\n",
      "Epoch 43/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8965 - accuracy: 0.2768 - top-5-accuracy: 0.8475\n",
      "Epoch 44/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.8987 - accuracy: 0.2881 - top-5-accuracy: 0.8814\n",
      "Epoch 45/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.9041 - accuracy: 0.2938 - top-5-accuracy: 0.8362\n",
      "Epoch 46/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8832 - accuracy: 0.2994 - top-5-accuracy: 0.8418\n",
      "Epoch 47/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.9037 - accuracy: 0.2599 - top-5-accuracy: 0.8475\n",
      "Epoch 48/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8965 - accuracy: 0.2881 - top-5-accuracy: 0.8249\n",
      "Epoch 49/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.8842 - accuracy: 0.2655 - top-5-accuracy: 0.8418\n",
      "Epoch 50/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8864 - accuracy: 0.2599 - top-5-accuracy: 0.8531\n",
      "Epoch 51/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.8677 - accuracy: 0.2994 - top-5-accuracy: 0.8305\n",
      "Epoch 52/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8876 - accuracy: 0.2825 - top-5-accuracy: 0.8418\n",
      "Epoch 53/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.8685 - accuracy: 0.3164 - top-5-accuracy: 0.8475\n",
      "Epoch 54/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8582 - accuracy: 0.3164 - top-5-accuracy: 0.8418\n",
      "Epoch 55/160\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 1.8765 - accuracy: 0.2994 - top-5-accuracy: 0.8249\n",
      "Epoch 56/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.8567 - accuracy: 0.3051 - top-5-accuracy: 0.8644\n",
      "Epoch 57/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8850 - accuracy: 0.2373 - top-5-accuracy: 0.8418\n",
      "Epoch 58/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.8583 - accuracy: 0.3051 - top-5-accuracy: 0.8588\n",
      "Epoch 59/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.8543 - accuracy: 0.3051 - top-5-accuracy: 0.8644\n",
      "Epoch 60/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8573 - accuracy: 0.3220 - top-5-accuracy: 0.8136\n",
      "Epoch 61/160\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 1.8434 - accuracy: 0.2938 - top-5-accuracy: 0.8418\n",
      "Epoch 62/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8475 - accuracy: 0.3164 - top-5-accuracy: 0.8588\n",
      "Epoch 63/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8322 - accuracy: 0.3164 - top-5-accuracy: 0.8305\n",
      "Epoch 64/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8583 - accuracy: 0.2938 - top-5-accuracy: 0.8362\n",
      "Epoch 65/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.8172 - accuracy: 0.2994 - top-5-accuracy: 0.8531\n",
      "Epoch 66/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.8308 - accuracy: 0.2712 - top-5-accuracy: 0.8588\n",
      "Epoch 67/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.8024 - accuracy: 0.3390 - top-5-accuracy: 0.8757\n",
      "Epoch 68/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.8257 - accuracy: 0.3107 - top-5-accuracy: 0.8644\n",
      "Epoch 69/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.7951 - accuracy: 0.3164 - top-5-accuracy: 0.8701\n",
      "Epoch 70/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.7775 - accuracy: 0.3446 - top-5-accuracy: 0.8757\n",
      "Epoch 71/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.7999 - accuracy: 0.3277 - top-5-accuracy: 0.8588\n",
      "Epoch 72/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.7912 - accuracy: 0.3446 - top-5-accuracy: 0.8757\n",
      "Epoch 73/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.7808 - accuracy: 0.3503 - top-5-accuracy: 0.8588\n",
      "Epoch 74/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.7543 - accuracy: 0.3672 - top-5-accuracy: 0.8588\n",
      "Epoch 75/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.7613 - accuracy: 0.3446 - top-5-accuracy: 0.8701\n",
      "Epoch 76/160\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 1.7516 - accuracy: 0.3446 - top-5-accuracy: 0.8757\n",
      "Epoch 77/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.7589 - accuracy: 0.3390 - top-5-accuracy: 0.8418\n",
      "Epoch 78/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.7444 - accuracy: 0.3672 - top-5-accuracy: 0.8757\n",
      "Epoch 79/160\n",
      "6/6 [==============================] - 1s 174ms/step - loss: 1.7449 - accuracy: 0.3503 - top-5-accuracy: 0.8701\n",
      "Epoch 80/160\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 1.7600 - accuracy: 0.3277 - top-5-accuracy: 0.8814\n",
      "Epoch 81/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.7118 - accuracy: 0.3616 - top-5-accuracy: 0.8531\n",
      "Epoch 82/160\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 1.7318 - accuracy: 0.3503 - top-5-accuracy: 0.8644\n",
      "Epoch 83/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.7879 - accuracy: 0.3277 - top-5-accuracy: 0.8644\n",
      "Epoch 84/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.7844 - accuracy: 0.3220 - top-5-accuracy: 0.8701\n",
      "Epoch 85/160\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 1.7202 - accuracy: 0.3842 - top-5-accuracy: 0.8701\n",
      "Epoch 86/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.6845 - accuracy: 0.3672 - top-5-accuracy: 0.8814\n",
      "Epoch 87/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.7043 - accuracy: 0.3672 - top-5-accuracy: 0.8870\n",
      "Epoch 88/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.7040 - accuracy: 0.3333 - top-5-accuracy: 0.8588\n",
      "Epoch 89/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.6346 - accuracy: 0.3842 - top-5-accuracy: 0.8757\n",
      "Epoch 90/160\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 1.6536 - accuracy: 0.4124 - top-5-accuracy: 0.8927\n",
      "Epoch 91/160\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 1.6227 - accuracy: 0.4068 - top-5-accuracy: 0.8757\n",
      "Epoch 92/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.6662 - accuracy: 0.3955 - top-5-accuracy: 0.8927\n",
      "Epoch 93/160\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 1.6395 - accuracy: 0.3955 - top-5-accuracy: 0.8588\n",
      "Epoch 94/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.6167 - accuracy: 0.4237 - top-5-accuracy: 0.8870\n",
      "Epoch 95/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.5982 - accuracy: 0.4181 - top-5-accuracy: 0.8757\n",
      "Epoch 96/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.5666 - accuracy: 0.4407 - top-5-accuracy: 0.8644\n",
      "Epoch 97/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.5046 - accuracy: 0.4633 - top-5-accuracy: 0.8870\n",
      "Epoch 98/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.5012 - accuracy: 0.4520 - top-5-accuracy: 0.8757\n",
      "Epoch 99/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.4978 - accuracy: 0.4463 - top-5-accuracy: 0.8814\n",
      "Epoch 100/160\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 1.5584 - accuracy: 0.4181 - top-5-accuracy: 0.8927\n",
      "Epoch 101/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.6185 - accuracy: 0.3672 - top-5-accuracy: 0.8927\n",
      "Epoch 102/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 1.5116 - accuracy: 0.4407 - top-5-accuracy: 0.8814\n",
      "Epoch 103/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.4774 - accuracy: 0.4802 - top-5-accuracy: 0.8983\n",
      "Epoch 104/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.4657 - accuracy: 0.4746 - top-5-accuracy: 0.9040\n",
      "Epoch 105/160\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 1.4398 - accuracy: 0.4802 - top-5-accuracy: 0.8983\n",
      "Epoch 106/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.4959 - accuracy: 0.4859 - top-5-accuracy: 0.9096\n",
      "Epoch 107/160\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 1.5323 - accuracy: 0.4237 - top-5-accuracy: 0.9209\n",
      "Epoch 108/160\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 1.5061 - accuracy: 0.4463 - top-5-accuracy: 0.8927\n",
      "Epoch 109/160\n",
      "6/6 [==============================] - 1s 175ms/step - loss: 1.4767 - accuracy: 0.4915 - top-5-accuracy: 0.9040\n",
      "Epoch 110/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 1.3860 - accuracy: 0.5367 - top-5-accuracy: 0.9379\n",
      "Epoch 111/160\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 1.3094 - accuracy: 0.5650 - top-5-accuracy: 0.8983\n",
      "Epoch 112/160\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 1.2945 - accuracy: 0.5367 - top-5-accuracy: 0.8870\n",
      "Epoch 113/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.3058 - accuracy: 0.5876 - top-5-accuracy: 0.9040\n",
      "Epoch 114/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 1.2821 - accuracy: 0.5537 - top-5-accuracy: 0.9322\n",
      "Epoch 115/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 1.2859 - accuracy: 0.5706 - top-5-accuracy: 0.9266\n",
      "Epoch 116/160\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 1.3226 - accuracy: 0.5254 - top-5-accuracy: 0.9322\n",
      "Epoch 117/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.2399 - accuracy: 0.5706 - top-5-accuracy: 0.9040\n",
      "Epoch 118/160\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 1.1823 - accuracy: 0.5763 - top-5-accuracy: 0.9492\n",
      "Epoch 119/160\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 1.2229 - accuracy: 0.5367 - top-5-accuracy: 0.9322\n",
      "Epoch 120/160\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 1.1449 - accuracy: 0.6045 - top-5-accuracy: 0.9379\n",
      "Epoch 121/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.0450 - accuracy: 0.6497 - top-5-accuracy: 0.9379\n",
      "Epoch 122/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.0325 - accuracy: 0.6328 - top-5-accuracy: 0.9435\n",
      "Epoch 123/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.0724 - accuracy: 0.6215 - top-5-accuracy: 0.9605\n",
      "Epoch 124/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 1.0685 - accuracy: 0.5989 - top-5-accuracy: 0.9435\n",
      "Epoch 125/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 1.0253 - accuracy: 0.6667 - top-5-accuracy: 0.9153\n",
      "Epoch 126/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.9808 - accuracy: 0.6328 - top-5-accuracy: 0.9661\n",
      "Epoch 127/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.9588 - accuracy: 0.6554 - top-5-accuracy: 0.9492\n",
      "Epoch 128/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.9252 - accuracy: 0.6723 - top-5-accuracy: 0.9548\n",
      "Epoch 129/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.9269 - accuracy: 0.6836 - top-5-accuracy: 0.9661\n",
      "Epoch 130/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.8581 - accuracy: 0.6893 - top-5-accuracy: 0.9661\n",
      "Epoch 131/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 155ms/step - loss: 0.9744 - accuracy: 0.6554 - top-5-accuracy: 0.9605\n",
      "Epoch 132/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.9859 - accuracy: 0.6441 - top-5-accuracy: 0.9435\n",
      "Epoch 133/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.9499 - accuracy: 0.6554 - top-5-accuracy: 0.9492\n",
      "Epoch 134/160\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.8857 - accuracy: 0.6610 - top-5-accuracy: 0.9605\n",
      "Epoch 135/160\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.8052 - accuracy: 0.7062 - top-5-accuracy: 0.9774\n",
      "Epoch 136/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.7711 - accuracy: 0.7458 - top-5-accuracy: 0.9887\n",
      "Epoch 137/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.8104 - accuracy: 0.7232 - top-5-accuracy: 0.9605\n",
      "Epoch 138/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.7404 - accuracy: 0.7514 - top-5-accuracy: 0.9774\n",
      "Epoch 139/160\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 0.7673 - accuracy: 0.7514 - top-5-accuracy: 0.9718\n",
      "Epoch 140/160\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.7382 - accuracy: 0.7288 - top-5-accuracy: 0.9718\n",
      "Epoch 141/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.7544 - accuracy: 0.7740 - top-5-accuracy: 0.9774\n",
      "Epoch 142/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.7235 - accuracy: 0.7627 - top-5-accuracy: 0.9718\n",
      "Epoch 143/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.5960 - accuracy: 0.8079 - top-5-accuracy: 0.9831\n",
      "Epoch 144/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5864 - accuracy: 0.8249 - top-5-accuracy: 0.9887\n",
      "Epoch 145/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.4939 - accuracy: 0.8305 - top-5-accuracy: 0.9944\n",
      "Epoch 146/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.4984 - accuracy: 0.8588 - top-5-accuracy: 0.9774\n",
      "Epoch 147/160\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 0.4047 - accuracy: 0.8757 - top-5-accuracy: 0.9944\n",
      "Epoch 148/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4846 - accuracy: 0.8418 - top-5-accuracy: 1.0000\n",
      "Epoch 149/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4716 - accuracy: 0.8757 - top-5-accuracy: 0.9944\n",
      "Epoch 150/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.5617 - accuracy: 0.8249 - top-5-accuracy: 0.9944\n",
      "Epoch 151/160\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 0.5807 - accuracy: 0.7853 - top-5-accuracy: 0.9944\n",
      "Epoch 152/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5600 - accuracy: 0.7966 - top-5-accuracy: 0.9774\n",
      "Epoch 153/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4149 - accuracy: 0.8870 - top-5-accuracy: 0.9944\n",
      "Epoch 154/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.4264 - accuracy: 0.8757 - top-5-accuracy: 1.0000\n",
      "Epoch 155/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.6282 - accuracy: 0.7853 - top-5-accuracy: 0.9887\n",
      "Epoch 156/160\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.6714 - accuracy: 0.8079 - top-5-accuracy: 0.9774\n",
      "Epoch 157/160\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.7865 - accuracy: 0.7401 - top-5-accuracy: 0.9718\n",
      "Epoch 158/160\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.8456 - accuracy: 0.7514 - top-5-accuracy: 0.9774\n",
      "Epoch 159/160\n",
      "6/6 [==============================] - 1s 172ms/step - loss: 0.5229 - accuracy: 0.7797 - top-5-accuracy: 1.0000\n",
      "Epoch 160/160\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.4717 - accuracy: 0.8644 - top-5-accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 71ms/step - loss: 3.9497 - accuracy: 0.1500 - top-5-accuracy: 0.6500\n",
      "Test accuracy: 15.0%\n",
      "Test top 5 accuracy: 65.0%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE, strides=STRIDES\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 30, 226, 22  0           []                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " tubelet_embedding_1 (TubeletEm  (None, 216, 128)    4147328     ['input_2[0][0]']                \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " positional_encoder (Positional  (None, 216, 128)    27648       ['tubelet_embedding_1[0][0]']    \n",
      " Encoder)                                                                                         \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 216, 128)    256         ['positional_encoder[0][0]']     \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 216, 128)    66048       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 216, 128)     0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'positional_encoder[0][0]']     \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 216, 128)    256         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 216, 128)     131712      ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 216, 128)     0           ['sequential[0][0]',             \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 216, 128)    256         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 216, 128)    66048       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 216, 128)     0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 216, 128)    256         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 216, 128)     0           ['sequential_1[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 216, 128)    256         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 216, 128)    66048       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 216, 128)     0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 216, 128)    256         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 216, 128)     0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 216, 128)    256         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 216, 128)    66048       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 216, 128)     0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 216, 128)    256         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 216, 128)     0           ['sequential_3[0][0]',           \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 216, 128)    256         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_head_attention_4 (MultiH  (None, 216, 128)    66048       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 216, 128)     0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 216, 128)    256         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 216, 128)     0           ['sequential_4[0][0]',           \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 216, 128)    256         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 216, 128)    66048       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 216, 128)     0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 216, 128)    256         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 216, 128)     0           ['sequential_5[0][0]',           \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 216, 128)    256         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 216, 128)    66048       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 216, 128)     0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 216, 128)    256         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 216, 128)     0           ['sequential_6[0][0]',           \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 216, 128)    256         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 216, 128)    66048       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 216, 128)     0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 216, 128)    256         ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_7 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 216, 128)     0           ['sequential_7[0][0]',           \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 216, 128)    256         ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 216, 128)    66048       ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 216, 128)     0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 216, 128)    256         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 216, 128)     0           ['sequential_8[0][0]',           \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 216, 128)    256         ['add_17[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 216, 128)    66048       ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 216, 128)     0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 216, 128)    256         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 216, 128)     131712      ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 216, 128)     0           ['sequential_9[0][0]',           \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 216, 128)    256         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['layer_normalization_20[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            1032        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,158,984\n",
      "Trainable params: 6,158,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 30, 226, 226, 1)\n",
      "tf.Tensor(\n",
      "[0. 0. 6. 0. 5. 2. 7. 2. 2. 7. 7. 5. 7. 7. 4. 4. 4. 2. 2. 6. 7. 7. 7. 4.\n",
      " 6. 4. 3. 3. 6. 6. 5. 6.], shape=(32,), dtype=float32)\n",
      "1/1 [==============================] - 1s 754ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e10fa8d7254fb5a0d5ad405e7a018b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: 0 | P: 7'\"), Box(children=(Image(value=b'GIF89a\\xe2\\x00\\xe2\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_SAMPLES_VIZ = 25\n",
    "testsamples, labels = next(iter(testloader))\n",
    "print(testsamples.shape)\n",
    "print(labels)\n",
    "\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = str(ground_truths[i])\n",
    "    pred_class = str(preds[i])\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 53ms/step\n"
     ]
    }
   ],
   "source": [
    "pred= model.predict(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "for test in testloader:\n",
    "    testsamples, labels = next(iter(testloader))\n",
    "    for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "        # Get model prediction\n",
    "        output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "        pred = np.argmax(output, axis=0)\n",
    "\n",
    "        ground_truths.append(label.numpy().astype(\"int\"))\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "con_mat = confusion_matrix(test_labels, preds[:100])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.10      0.11        10\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.17      0.17      0.17        12\n",
      "           3       0.00      0.00      0.00        11\n",
      "           4       0.00      0.00      0.00        19\n",
      "           5       0.07      0.43      0.13         7\n",
      "           6       0.00      0.00      0.00        16\n",
      "           7       0.18      0.35      0.24        17\n",
      "\n",
      "    accuracy                           0.12       100\n",
      "   macro avg       0.07      0.13      0.08       100\n",
      "weighted avg       0.07      0.12      0.08       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacobo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jacobo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jacobo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, preds[:100],labels=[0,1,2,3,4,5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bovine-gpu",
   "language": "python",
   "name": "bovine-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
