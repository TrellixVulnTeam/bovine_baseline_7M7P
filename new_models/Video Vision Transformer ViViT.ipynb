{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bcbb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc1bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70862a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "571d42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"Utility function to download the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata.\n",
    "    \"\"\"\n",
    "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get videos\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos = data[\"test_images\"]\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return (\n",
    "        (train_videos, train_labels),\n",
    "        (valid_videos, valid_labels),\n",
    "        (test_videos, test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the metadata of the dataset\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "\n",
    "# Get the dataset\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos, test_labels) = prepared_dataset[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9db6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 28, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85a9467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fe4701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_input=train_videos[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0250d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_input=tf.image.convert_image_dtype(your_input[..., tf.newaxis],tf.float32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f19d2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_input=your_input[None,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7453351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 28, 28, 28, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e05d99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b5344a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import getsizeof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bbf79ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(trainloader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23a17887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf.data.Dataset.from_tensor_slices([1,2,3], 'a')\n",
    "list(d.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f826b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "\n",
    "        return flattened_patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b03c566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape=(28,28,28,1))\n",
    "out = TubeletEmbedding(\n",
    "    embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    ")(inp)\n",
    "model = Model(inp, out)\n",
    "\n",
    "output = model.predict(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae70ca38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 27, 128)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eca58806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _, num_tokens, _ =output.shape\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62051e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42fc772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c58e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "31/31 [==============================] - 9s 80ms/step - loss: 2.4121 - accuracy: 0.1368 - top-5-accuracy: 0.6029 - val_loss: 2.3017 - val_accuracy: 0.1801 - val_top-5-accuracy: 0.5155\n",
      "Epoch 2/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 2.1768 - accuracy: 0.1914 - top-5-accuracy: 0.7027 - val_loss: 1.9054 - val_accuracy: 0.2547 - val_top-5-accuracy: 0.7950\n",
      "Epoch 3/60\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 2.0048 - accuracy: 0.2274 - top-5-accuracy: 0.7809 - val_loss: 1.7339 - val_accuracy: 0.3354 - val_top-5-accuracy: 0.8199\n",
      "Epoch 4/60\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 1.8662 - accuracy: 0.3128 - top-5-accuracy: 0.8457 - val_loss: 1.5249 - val_accuracy: 0.4472 - val_top-5-accuracy: 0.8696\n",
      "Epoch 5/60\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 1.6443 - accuracy: 0.3601 - top-5-accuracy: 0.8930 - val_loss: 1.2216 - val_accuracy: 0.5342 - val_top-5-accuracy: 0.9689\n",
      "Epoch 6/60\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 1.4679 - accuracy: 0.4146 - top-5-accuracy: 0.9218 - val_loss: 1.1289 - val_accuracy: 0.5155 - val_top-5-accuracy: 0.9752\n",
      "Epoch 7/60\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 1.3883 - accuracy: 0.4280 - top-5-accuracy: 0.9311 - val_loss: 1.0756 - val_accuracy: 0.4969 - val_top-5-accuracy: 0.9689\n",
      "Epoch 8/60\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 1.3523 - accuracy: 0.4578 - top-5-accuracy: 0.9331 - val_loss: 1.0521 - val_accuracy: 0.5404 - val_top-5-accuracy: 0.9689\n",
      "Epoch 9/60\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 1.1558 - accuracy: 0.5628 - top-5-accuracy: 0.9588 - val_loss: 0.8427 - val_accuracy: 0.7019 - val_top-5-accuracy: 0.9814\n",
      "Epoch 10/60\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 1.1155 - accuracy: 0.5823 - top-5-accuracy: 0.9558 - val_loss: 0.9537 - val_accuracy: 0.6398 - val_top-5-accuracy: 0.9876\n",
      "Epoch 11/60\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 1.0435 - accuracy: 0.6163 - top-5-accuracy: 0.9588 - val_loss: 0.8043 - val_accuracy: 0.6708 - val_top-5-accuracy: 0.9876\n",
      "Epoch 12/60\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 1.0153 - accuracy: 0.6080 - top-5-accuracy: 0.9712 - val_loss: 0.6631 - val_accuracy: 0.7267 - val_top-5-accuracy: 0.9938\n",
      "Epoch 13/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.8153 - accuracy: 0.6831 - top-5-accuracy: 0.9763 - val_loss: 0.8227 - val_accuracy: 0.6646 - val_top-5-accuracy: 0.9876\n",
      "Epoch 14/60\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.8220 - accuracy: 0.7109 - top-5-accuracy: 0.9846 - val_loss: 0.4813 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
      "Epoch 15/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.7374 - accuracy: 0.7160 - top-5-accuracy: 0.9877 - val_loss: 0.5143 - val_accuracy: 0.8199 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/60\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 0.6101 - accuracy: 0.7829 - top-5-accuracy: 0.9918 - val_loss: 0.3912 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/60\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.6031 - accuracy: 0.7840 - top-5-accuracy: 0.9918 - val_loss: 0.3886 - val_accuracy: 0.8634 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/60\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 0.5486 - accuracy: 0.8128 - top-5-accuracy: 0.9897 - val_loss: 0.4577 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
      "Epoch 19/60\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.4385 - accuracy: 0.8488 - top-5-accuracy: 0.9990 - val_loss: 0.4654 - val_accuracy: 0.8323 - val_top-5-accuracy: 0.9938\n",
      "Epoch 20/60\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.4563 - accuracy: 0.8354 - top-5-accuracy: 0.9959 - val_loss: 0.3281 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.3633 - accuracy: 0.8673 - top-5-accuracy: 0.9969 - val_loss: 0.3145 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.3974 - accuracy: 0.8539 - top-5-accuracy: 0.9979 - val_loss: 0.3609 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 23/60\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.3248 - accuracy: 0.8909 - top-5-accuracy: 0.9990 - val_loss: 0.3447 - val_accuracy: 0.8634 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/60\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.2850 - accuracy: 0.9084 - top-5-accuracy: 0.9979 - val_loss: 0.2525 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.2336 - accuracy: 0.9136 - top-5-accuracy: 1.0000 - val_loss: 0.2885 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/60\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 0.2438 - accuracy: 0.9218 - top-5-accuracy: 0.9979 - val_loss: 0.2003 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/60\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2711 - accuracy: 0.9064 - top-5-accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.1648 - accuracy: 0.9444 - top-5-accuracy: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1825 - accuracy: 0.9424 - top-5-accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/60\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2431 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 0.4140 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9938\n",
      "Epoch 31/60\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.2053 - accuracy: 0.9342 - top-5-accuracy: 1.0000 - val_loss: 0.3037 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 32/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.1549 - accuracy: 0.9486 - top-5-accuracy: 0.9990 - val_loss: 0.2440 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 33/60\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.1670 - accuracy: 0.9475 - top-5-accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.8634 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.1687 - accuracy: 0.9444 - top-5-accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.1956 - accuracy: 0.9424 - top-5-accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0854 - accuracy: 0.9733 - top-5-accuracy: 0.9990 - val_loss: 0.2744 - val_accuracy: 0.9317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0617 - accuracy: 0.9815 - top-5-accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0549 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 39/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0538 - accuracy: 0.9835 - top-5-accuracy: 1.0000 - val_loss: 0.2614 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0332 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.3142 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0253 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.3780 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0224 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.2569 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0197 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.2819 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0393 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 45/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0190 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.2202 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0093 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0133 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0139 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.2694 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0054 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.9317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/60\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0416 - accuracy: 0.9856 - top-5-accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.8820 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0389 - accuracy: 0.9877 - top-5-accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0372 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0248 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 54/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0272 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.2325 - val_accuracy: 0.9503 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/60\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 0.0100 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/60\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 0.0104 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0044 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.2192 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0145 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.3286 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.0278 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 60/60\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0216 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.3808 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 1.0665 - accuracy: 0.7820 - top-5-accuracy: 0.9705\n",
      "Test accuracy: 78.2%\n",
      "Test top 5 accuracy: 97.05%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8884ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 28, 28, 28,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " tubelet_embedding_3 (TubeletEm  (None, 27, 128)     65664       ['input_4[0][0]']                \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " positional_encoder_1 (Position  (None, 27, 128)     3456        ['tubelet_embedding_3[0][0]']    \n",
      " alEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 27, 128)     256         ['positional_encoder_1[0][0]']   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 27, 128)     66048       ['layer_normalization_17[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 27, 128)      0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'positional_encoder_1[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 27, 128)     256         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, 27, 128)      131712      ['layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 27, 128)      0           ['sequential_8[0][0]',           \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 27, 128)     256         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 27, 128)     66048       ['layer_normalization_19[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 27, 128)      0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 27, 128)     256         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 27, 128)      131712      ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 27, 128)      0           ['sequential_9[0][0]',           \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 27, 128)     256         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 27, 128)     66048       ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 27, 128)      0           ['multi_head_attention_10[0][0]',\n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 27, 128)     256         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 27, 128)      0           ['sequential_10[0][0]',          \n",
      "                                                                  'add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 27, 128)     256         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 27, 128)     66048       ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 27, 128)      0           ['multi_head_attention_11[0][0]',\n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 27, 128)     256         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 27, 128)      0           ['sequential_11[0][0]',          \n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 27, 128)     256         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_head_attention_12 (Multi  (None, 27, 128)     66048       ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 27, 128)      0           ['multi_head_attention_12[0][0]',\n",
      "                                                                  'add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 27, 128)     256         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 27, 128)      0           ['sequential_12[0][0]',          \n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 27, 128)     256         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 27, 128)     66048       ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 27, 128)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 27, 128)     256         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_13 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 27, 128)      0           ['sequential_13[0][0]',          \n",
      "                                                                  'add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 27, 128)     256         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 27, 128)     66048       ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 27, 128)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 27, 128)     256         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_14 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 27, 128)      0           ['sequential_14[0][0]',          \n",
      "                                                                  'add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 27, 128)     256         ['add_29[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 27, 128)     66048       ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 27, 128)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 27, 128)     256         ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_15 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 27, 128)      0           ['sequential_15[0][0]',          \n",
      "                                                                  'add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 27, 128)     256         ['add_31[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['layer_normalization_33[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 11)           1419        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,656,971\n",
      "Trainable params: 1,656,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_VIZ = 25\n",
    "testsamples, labels = next(iter(testloader))\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = info[\"label\"][str(ground_truths[i])]\n",
    "    pred_class = info[\"label\"][str(preds[i])]\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82983bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ccb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
