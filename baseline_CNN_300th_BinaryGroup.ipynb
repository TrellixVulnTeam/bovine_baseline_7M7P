{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data, get_test_data, WeightedClassificationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_train, labels_train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slices(num_rows, num_columns, width, height, data):\n",
    "    \n",
    "    data = np.transpose(data)\n",
    "    data = np.reshape(data, (num_rows, num_columns, width, height))\n",
    "    rows_data, columns_data = data.shape[0], data.shape[1]\n",
    "    heights = [slc[0].shape[0] for slc in data]\n",
    "    widths = [slc.shape[1] for slc in data[0]]\n",
    "    fig_width = 12.0\n",
    "    fig_height = fig_width * sum(heights) / sum(widths)\n",
    "    f, axarr = plt.subplots(\n",
    "        rows_data,\n",
    "        columns_data,\n",
    "        figsize=(fig_width, fig_height),\n",
    "        gridspec_kw={\"height_ratios\": heights},\n",
    "    )\n",
    "    for i in range(rows_data):\n",
    "        for j in range(columns_data):\n",
    "            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_frames(video):\n",
    "    res=[]\n",
    "    for frame in video:\n",
    "        resized_img=Image.fromarray(frame).resize((224,224))\n",
    "        res.append(np.array(resized_img))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data set: each observation now correponds the nth observation\n",
    "def nth_frame(videolist, n):\n",
    "    n = 300\n",
    "    nth_frames=[]\n",
    "    i=0\n",
    "    for video in videolist:\n",
    "        i+=1\n",
    "        time= video.frame_times[n-1]\n",
    "        frame= video.read_frame(time)\n",
    "        nth_frames.append(frame)\n",
    "    return nth_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(startFrame, endFrame, x, y):\n",
    "    xres= nth_frame(x,startFrame)\n",
    "    yres= np.array(y)\n",
    "    for i in range(startFrame+1, endFrame+1):\n",
    "        temp= nth_frame(x,i)\n",
    "        xres+= temp\n",
    "        yres= np.append(yres,np.array(y))\n",
    "    return xres,yres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "builtx, builty= create_dataset(300,300, videos_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_classifier=np.array(builtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_classifier=resize_frames(X_for_classifier)\n",
    "y_for_classifier= builty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 177, 224, 224)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_for_classifier[None, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp fix to make it work with MobileNetV2\n",
    "\n",
    "grayscale_batch=X_for_classifier\n",
    "rgb_batch = np.repeat(grayscale_batch[...,np.newaxis], 3, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_classifier= rgb_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 224, 224, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_for_classifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_int(argument):\n",
    "    switcher = {\n",
    "        'A':0,\n",
    "        'B':0,\n",
    "        'C':0,\n",
    "        'D':0,\n",
    "        'E':1,\n",
    "        'F':1,\n",
    "        'G':1,\n",
    "        'H':1,\n",
    "    }\n",
    " \n",
    "    # get() method of dictionary data type returns\n",
    "    # value of passed argument if it is present\n",
    "    # in dictionary otherwise second argument will\n",
    "    # be assigned as default value of passed argument\n",
    "    return switcher.get(argument, \"nothing\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "func=np.vectorize(class_to_int)\n",
    "y_for_classifier=func(y_for_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_for_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new Baseline test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predefined pred times:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ensuring that only frames up to pred_times are used for model training and testing. None is the whole 300 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_times = [27, 32, 37, 40, 44, 48, 53, 58, 63, 94, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image shape for classifier\n",
    "IMG_SHAPE = (224,224,3)\n",
    "\n",
    "# Building of a classification model\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "base_model.trainable = False\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(2, activation=\"sigmoid\")\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255, \n",
    "        rotation_range=30, fill_mode='nearest',\n",
    "        width_shift_range=0.1, height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        brightness_range=[0.8,1.2] #above 1 light , below darkens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_classifier_augmented= datagen.flow(X_for_classifier, y_for_classifier, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 4s 68ms/step - loss: 1.1660 - sparse_categorical_accuracy: 0.4068\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.9980 - sparse_categorical_accuracy: 0.4237\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.8564 - sparse_categorical_accuracy: 0.4237\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.8062 - sparse_categorical_accuracy: 0.5480\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.7446 - sparse_categorical_accuracy: 0.5254\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.7493 - sparse_categorical_accuracy: 0.5028\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.7659 - sparse_categorical_accuracy: 0.5706\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.7224 - sparse_categorical_accuracy: 0.5989\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7681 - sparse_categorical_accuracy: 0.5367\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7329 - sparse_categorical_accuracy: 0.5876\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7504 - sparse_categorical_accuracy: 0.5593\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7767 - sparse_categorical_accuracy: 0.5537\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7385 - sparse_categorical_accuracy: 0.5537\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7143 - sparse_categorical_accuracy: 0.5876\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6722 - sparse_categorical_accuracy: 0.5989\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7032 - sparse_categorical_accuracy: 0.5819\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.5876\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6321 - sparse_categorical_accuracy: 0.6497\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6568 - sparse_categorical_accuracy: 0.6158\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6756 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6449 - sparse_categorical_accuracy: 0.6554\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6114 - sparse_categorical_accuracy: 0.6215\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6297 - sparse_categorical_accuracy: 0.6554\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6511 - sparse_categorical_accuracy: 0.6497\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.5631 - sparse_categorical_accuracy: 0.7232\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6158 - sparse_categorical_accuracy: 0.6554\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6244 - sparse_categorical_accuracy: 0.6893\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6144 - sparse_categorical_accuracy: 0.6949\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5879 - sparse_categorical_accuracy: 0.7006\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5895 - sparse_categorical_accuracy: 0.6836\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5937 - sparse_categorical_accuracy: 0.6723\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6204 - sparse_categorical_accuracy: 0.6215\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5728 - sparse_categorical_accuracy: 0.6949\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5334 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5598 - sparse_categorical_accuracy: 0.7062\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5749 - sparse_categorical_accuracy: 0.7062\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5698 - sparse_categorical_accuracy: 0.6780\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5365 - sparse_categorical_accuracy: 0.7062\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5295 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5728 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5463 - sparse_categorical_accuracy: 0.7288\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5306 - sparse_categorical_accuracy: 0.7458\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5142 - sparse_categorical_accuracy: 0.7401\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5130 - sparse_categorical_accuracy: 0.7571\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4928 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5315 - sparse_categorical_accuracy: 0.7458\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4962 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5055 - sparse_categorical_accuracy: 0.7571\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4988 - sparse_categorical_accuracy: 0.7740\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4765 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.5021 - sparse_categorical_accuracy: 0.7514\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.7966\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4821 - sparse_categorical_accuracy: 0.7514\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4884 - sparse_categorical_accuracy: 0.7571\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.4857 - sparse_categorical_accuracy: 0.7345\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4643 - sparse_categorical_accuracy: 0.7966\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4689 - sparse_categorical_accuracy: 0.7910\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4759 - sparse_categorical_accuracy: 0.7458\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4655 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.4414 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5020 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4782 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.4705 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4337 - sparse_categorical_accuracy: 0.8023\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4636 - sparse_categorical_accuracy: 0.7740\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4554 - sparse_categorical_accuracy: 0.7966\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4354 - sparse_categorical_accuracy: 0.8079\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4713 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4833 - sparse_categorical_accuracy: 0.7797\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8305\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.4414 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.3989 - sparse_categorical_accuracy: 0.8249\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.4068 - sparse_categorical_accuracy: 0.8136\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4494 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4184 - sparse_categorical_accuracy: 0.8305\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4431 - sparse_categorical_accuracy: 0.7684\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.4160 - sparse_categorical_accuracy: 0.8079\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.4019 - sparse_categorical_accuracy: 0.8136\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4136 - sparse_categorical_accuracy: 0.8079\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4023 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4029 - sparse_categorical_accuracy: 0.7966\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4446 - sparse_categorical_accuracy: 0.7797\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4163 - sparse_categorical_accuracy: 0.8249\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4042 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4480 - sparse_categorical_accuracy: 0.8079\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4012 - sparse_categorical_accuracy: 0.8079\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4284 - sparse_categorical_accuracy: 0.8136\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3890 - sparse_categorical_accuracy: 0.8475\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4194 - sparse_categorical_accuracy: 0.7966\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4295 - sparse_categorical_accuracy: 0.7740\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3994 - sparse_categorical_accuracy: 0.8644\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.3834 - sparse_categorical_accuracy: 0.8362\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.3696 - sparse_categorical_accuracy: 0.8531\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.3922 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.3890 - sparse_categorical_accuracy: 0.8531\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4139 - sparse_categorical_accuracy: 0.8023\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3950 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3803 - sparse_categorical_accuracy: 0.8192\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3815 - sparse_categorical_accuracy: 0.8362\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4030 - sparse_categorical_accuracy: 0.8249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d03624b0d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_for_classifier,y_for_classifier, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell to Load a previously saved model\n",
    "#MODEL_NAME_FOR_PREDICTION = \"classifier3\"\n",
    "#model = tf.keras.models.load_model(os.path.join(MODELS_FOLDER, MODEL_NAME_FOR_PREDICTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image, model):\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((224,224))\n",
    "    image = np.array(image)\n",
    "    image = tf.reshape(image, (1,224,224))\n",
    "    pred = model.predict(image)\n",
    "    return np.argmax(pred), np.max(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_test, labels_test  = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtx, builty= create_dataset(300,300, videos_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_for_classifier = resize_frames(builtx)\n",
    "ytest_for_classifier = np.array(builty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'G', 'A', 'F', 'C', 'H', 'C', 'C', 'H', 'H', 'F', 'H',\n",
       "       'H', 'E', 'E', 'E', 'C', 'C', 'G', 'H', 'H', 'H', 'E', 'G', 'E',\n",
       "       'D', 'D', 'G', 'G', 'F', 'G', 'E', 'C', 'E', 'A', 'E', 'E', 'C',\n",
       "       'G', 'G', 'E', 'A', 'D', 'D', 'D', 'A', 'A', 'F', 'C', 'B', 'F',\n",
       "       'C', 'B', 'B', 'B', 'C', 'E', 'E', 'B', 'E', 'D', 'G', 'E', 'B',\n",
       "       'G', 'E', 'D', 'D', 'D', 'A', 'E', 'E', 'C', 'G', 'H', 'H', 'H',\n",
       "       'F', 'H', 'G', 'B', 'F', 'H', 'H', 'G', 'G', 'E', 'C', 'H', 'A',\n",
       "       'A', 'H', 'B', 'G', 'G', 'H', 'E', 'D', 'D'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_for_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_batch=Xtest_for_classifier\n",
    "rgb_batch = np.repeat(grayscale_batch[..., np.newaxis], 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_for_classifier=func(ytest_for_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_for_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_for_classifier= rgb_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 224, 224, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_for_classifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 81ms/step - loss: 0.6758 - sparse_categorical_accuracy: 0.6600\n",
      "Test accuracy : 0.6600000262260437\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(Xtest_for_classifier, ytest_for_classifier)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(Xtest_for_classifier)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_for_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "con_mat = confusion_matrix(ytest_for_classifier, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.63      0.60        41\n",
      "           1       0.73      0.68      0.70        59\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.65      0.66      0.65       100\n",
      "weighted avg       0.67      0.66      0.66       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest_for_classifier, preds,labels=[0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bovine-gpu",
   "language": "python",
   "name": "bovine-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
